package com.github.hakenadu.javalangchain.chains.llm.openai;

import java.util.Map;

import com.fasterxml.jackson.annotation.JsonProperty;

/**
 * Parameters for calling an OpenAI Chat Model
 * 
 * @see https://platform.openai.com/docs/api-reference/chat/create
 */
public class OpenAiChatParameters {

	/**
	 * <h1>From
	 * https://github.com/openai/openai-openapi/blob/master/openapi.yaml</h1>
	 * 
	 * Number between -2.0 and 2.0. Positive values penalize new tokens based on
	 * their existing frequency in the text so far, decreasing the model's
	 * likelihood to repeat the same line verbatim.
	 */
	@JsonProperty("frequence_penalty")
	private Double frequencePenalty;

	/**
	 * <h1>From
	 * https://github.com/openai/openai-openapi/blob/master/openapi.yaml</h1>
	 * 
	 * Modify the likelihood of specified tokens appearing in the completion.
	 * Accepts a json object that maps tokens (specified by their token ID in the
	 * tokenizer) to an associated bias value from -100 to 100. Mathematically, the
	 * bias is added to the logits generated by the model prior to sampling. The
	 * exact effect will vary per model, but values between -1 and 1 should decrease
	 * or increase likelihood of selection; values like -100 or 100 should result in
	 * a ban or exclusive selection of the relevant token.
	 */
	private Map<Integer, Integer> logitBias;

	/**
	 * <h1>From
	 * https://github.com/openai/openai-openapi/blob/master/openapi.yaml</h1>
	 * 
	 * The maximum number of tokens allowed for the generated answer. By default,
	 * the number of tokens the model can return will be (4096 - prompt tokens).
	 */
	@JsonProperty("max_tokens")
	private Integer maxTokens;

	/**
	 * <h1>From
	 * https://github.com/openai/openai-openapi/blob/master/openapi.yaml</h1>
	 * 
	 * ID of the model to use. Currently, only `gpt-3.5-turbo` and
	 * `gpt-3.5-turbo-0301` are supported.
	 */
	private String model;

	/**
	 * <h1>From
	 * https://github.com/openai/openai-openapi/blob/master/openapi.yaml</h1>
	 * 
	 * How many completions to generate for each prompt.
	 **
	 * Note:** Because this parameter generates many completions, it can quickly
	 * consume your token quota. Use carefully and ensure that you have reasonable
	 * settings for `max_tokens` and `stop`.
	 */
	@JsonProperty
	private Integer n;
	/**
	 * <h1>From
	 * https://github.com/openai/openai-openapi/blob/master/openapi.yaml</h1>
	 * 
	 * Number between -2.0 and 2.0. Positive values penalize new tokens based on
	 * whether they appear in the text so far, increasing the model's likelihood to
	 * talk about new topics.
	 */
	@JsonProperty("precence_penalty")
	private Double presencePenalty;

	/**
	 * <h1>From
	 * https://github.com/openai/openai-openapi/blob/master/openapi.yaml</h1>
	 * 
	 * What sampling temperature to use, between 0 and 2. Higher values like 0.8
	 * will make the output more random, while lower values like 0.2 will make it
	 * more focused and deterministic.
	 * 
	 * We generally recommend altering this or `top_p` but not both.
	 */
	@JsonProperty
	private Integer temperature;

	public Double getFrequencePenalty() {
		return frequencePenalty;
	}

	public void setFrequencePenalty(final Double frequencePenalty) {
		this.frequencePenalty = frequencePenalty;
	}

	public Map<Integer, Integer> getLogitBias() {
		return logitBias;
	}

	public void setLogitBias(final Map<Integer, Integer> logitBias) {
		this.logitBias = logitBias;
	}

	public Integer getMaxTokens() {
		return maxTokens;
	}

	public void setMaxTokens(final Integer maxTokens) {
		this.maxTokens = maxTokens;
	}

	public String getModel() {
		return model;
	}

	public void setModel(final String model) {
		this.model = model;
	}

	public Integer getN() {
		return n;
	}

	public void setN(final Integer n) {
		this.n = n;
	}

	public Double getPresencePenalty() {
		return presencePenalty;
	}

	public void setPresencePenalty(final Double presencePenalty) {
		this.presencePenalty = presencePenalty;
	}

	public Integer getTemperature() {
		return temperature;
	}

	public void setTemperature(final Integer temperature) {
		this.temperature = temperature;
	}

	public void copyFrom(final OpenAiChatParameters parameters) {
		this.setFrequencePenalty(parameters.getFrequencePenalty());
		this.setLogitBias(parameters.getLogitBias());
		this.setMaxTokens(parameters.getMaxTokens());
		this.setModel(parameters.getModel());
		this.setN(parameters.getN());
		this.setPresencePenalty(parameters.getPresencePenalty());
		this.setTemperature(parameters.getTemperature());
	}
}
